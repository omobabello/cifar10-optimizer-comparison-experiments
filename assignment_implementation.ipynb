{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CN7023 - Individual Assignment: Image Multi-Class Classifier\n",
    "\n",
    "## Research Question:\n",
    "**How do different data augmentation techniques (rotation, horizontal flip, combined transformations) affect CNN classification performance and generalization on Fashion-MNIST?**\n",
    "\n",
    "## Experiments:\n",
    "1. **Baseline**: No augmentation\n",
    "2. **Rotation**: Random rotation (±30°)\n",
    "3. **Horizontal Flip**: Random horizontal flip\n",
    "4. **Combined**: Rotation + Flip + Random affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "# Data handling and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image configuration\n",
    "IMAGE_SIZE = 28  # Fashion-MNIST is 28x28 (we'll keep original size)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 5  # Can increase if time permits\n",
    "\n",
    "# Model architecture parameters\n",
    "OUT_CHANNELS_1 = 16  # First conv layer output channels\n",
    "OUT_CHANNELS_2 = 32  # Second conv layer output channels\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models'\n",
    "RESULTS_DIR = '../results'\n",
    "PLOTS_DIR = '../results/plots'\n",
    "METRICS_DIR = '../results/metrics'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# Fashion-MNIST class names\n",
    "CLASS_NAMES = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network for Fashion-MNIST Classification\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv Layer 1: 1 -> 16 channels, 5x5 kernel, padding=2\n",
    "    - ReLU + MaxPool (2x2)\n",
    "    - Conv Layer 2: 16 -> 32 channels, 5x5 kernel, padding=2\n",
    "    - ReLU + MaxPool (2x2)\n",
    "    - Fully Connected: 32*7*7 -> 10 classes\n",
    "    \n",
    "    Channel width calculation:\n",
    "    - Input: 28x28\n",
    "    - After Conv1 (padding=2): 28x28\n",
    "    - After MaxPool1: 14x14\n",
    "    - After Conv2 (padding=2): 14x14\n",
    "    - After MaxPool2: 7x7\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, \n",
    "                              kernel_size=5, padding=2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, \n",
    "                              kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # Input size: out_2 * 7 * 7 (for 28x28 input)\n",
    "        self.fc1 = nn.Linear(out_2 * 7 * 7, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # First conv block\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        # Flatten and fully connected\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Test the model\n",
    "test_model = CNN(OUT_CHANNELS_1, OUT_CHANNELS_2)\n",
    "print(f\"Model architecture:\\n{test_model}\")\n",
    "print(f\"\\nTotal trainable parameters: {test_model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for different experiments\n",
    "\n",
    "# Baseline: Only normalization\n",
    "transform_baseline = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Rotation augmentation (±30 degrees)\n",
    "transform_rotation = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Horizontal flip augmentation\n",
    "transform_flip = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Combined augmentation (rotation + flip + affine)\n",
    "transform_combined = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Test/Validation transform (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "print(\"Data transformations defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "\n",
    "# Download and load training dataset (we'll use baseline transform for exploration)\n",
    "train_dataset_full = dsets.FashionMNIST(root=DATA_DIR, train=True, \n",
    "                                        download=True, transform=transform_baseline)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = dsets.FashionMNIST(root=DATA_DIR, train=False, \n",
    "                                  download=True, transform=transform_test)\n",
    "\n",
    "# Split training data into train and validation\n",
    "train_size = int(0.8 * len(train_dataset_full))  # 80% for training\n",
    "val_size = len(train_dataset_full) - train_size   # 20% for validation\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset_full, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "\n",
    "def show_data(data_sample, label):\n",
    "    \"\"\"Display a single image with its label\"\"\"\n",
    "    plt.imshow(data_sample.squeeze(), cmap='gray')\n",
    "    plt.title(f'Class: {CLASS_NAMES[label]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "def visualize_samples(dataset, n_samples=10):\n",
    "    \"\"\"Visualize one sample from each class\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Find one example of each class\n",
    "    samples_found = {}\n",
    "    for img, label in dataset:\n",
    "        if label not in samples_found:\n",
    "            samples_found[label] = img\n",
    "        if len(samples_found) == 10:\n",
    "            break\n",
    "    \n",
    "    # Plot samples\n",
    "    for idx in range(10):\n",
    "        if idx in samples_found:\n",
    "            axes[idx].imshow(samples_found[idx].squeeze(), cmap='gray')\n",
    "            axes[idx].set_title(f'{CLASS_NAMES[idx]}', fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'sample_images.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_samples(train_dataset_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distribution\n",
    "\n",
    "def analyze_class_distribution(dataset, title=\"Class Distribution\"):\n",
    "    \"\"\"Analyze and plot class distribution\"\"\"\n",
    "    labels = []\n",
    "    for _, label in dataset:\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Count occurrences\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.bar([CLASS_NAMES[i] for i in unique], counts)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, 'class_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for class_idx, count in zip(unique, counts):\n",
    "        print(f\"{CLASS_NAMES[class_idx]:15s}: {count:5d} samples ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return unique, counts\n",
    "\n",
    "# Analyze distribution\n",
    "analyze_class_distribution(train_dataset_full, \"Training Set Class Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                num_epochs, device, experiment_name=\"baseline\"):\n",
    "    \"\"\"\n",
    "    Train the model and track metrics\n",
    "    \n",
    "    Returns:\n",
    "        history: dict with training history (loss, accuracy per epoch)\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"\\nTraining model: {experiment_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        epoch_time = (datetime.now() - start_time).total_seconds()\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.2f}% | \"\n",
    "              f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training completed!\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and return detailed metrics\n",
    "    \n",
    "    Returns:\n",
    "        results: dict with accuracy, predictions, confusion matrix, etc.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "    class_report = classification_report(all_targets, all_predictions, \n",
    "                                        target_names=class_names, \n",
    "                                        output_dict=True)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, experiment_name):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy curves\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{experiment_name} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy', marker='o')\n",
    "    ax2.plot(history['val_acc'], label='Val Accuracy', marker='o')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title(f'{experiment_name} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f\"{experiment_name.lower().replace(' ', '_')}_training_curves.png\"\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, filename), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix, class_names, experiment_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix heatmap\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'{experiment_name} - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    filename = f\"{experiment_name.lower().replace(' ', '_')}_confusion_matrix.png\"\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, filename), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 1: Baseline (No Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1: BASELINE (NO AUGMENTATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets with baseline transform\n",
    "train_dataset_baseline = dsets.FashionMNIST(root=DATA_DIR, train=True, \n",
    "                                            download=True, transform=transform_baseline)\n",
    "\n",
    "# Split into train/val\n",
    "train_baseline, val_baseline = torch.utils.data.random_split(\n",
    "    train_dataset_baseline, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_baseline = torch.utils.data.DataLoader(train_baseline, \n",
    "                                                    batch_size=BATCH_SIZE, \n",
    "                                                    shuffle=True)\n",
    "val_loader_baseline = torch.utils.data.DataLoader(val_baseline, \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model_baseline = CNN(OUT_CHANNELS_1, OUT_CHANNELS_2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_baseline = optim.SGD(model_baseline.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train model\n",
    "history_baseline = train_model(model_baseline, train_loader_baseline, val_loader_baseline,\n",
    "                              criterion, optimizer_baseline, NUM_EPOCHS, \n",
    "                              device, \"Baseline\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_baseline, \"Baseline\")\n",
    "\n",
    "# Evaluate on test set\n",
    "results_baseline = evaluate_model(model_baseline, test_loader, device, CLASS_NAMES)\n",
    "\n",
    "print(f\"\\nBaseline Test Accuracy: {results_baseline['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_baseline['confusion_matrix'], CLASS_NAMES, \"Baseline\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_baseline.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_baseline.state_dict(),\n",
    "    'history': history_baseline,\n",
    "    'test_results': results_baseline\n",
    "}, os.path.join(MODELS_DIR, 'baseline_model.pth'))\n",
    "\n",
    "print(\"Baseline model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment 2: Rotation Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: ROTATION AUGMENTATION (±30°)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets with rotation transform\n",
    "train_dataset_rotation = dsets.FashionMNIST(root=DATA_DIR, train=True, \n",
    "                                           download=True, transform=transform_rotation)\n",
    "\n",
    "# Split into train/val (validation uses no augmentation)\n",
    "train_rotation, _ = torch.utils.data.random_split(\n",
    "    train_dataset_rotation, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders (reuse validation loader from baseline)\n",
    "train_loader_rotation = torch.utils.data.DataLoader(train_rotation, \n",
    "                                                    batch_size=BATCH_SIZE, \n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model_rotation = CNN(OUT_CHANNELS_1, OUT_CHANNELS_2)\n",
    "optimizer_rotation = optim.SGD(model_rotation.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train model\n",
    "history_rotation = train_model(model_rotation, train_loader_rotation, val_loader_baseline,\n",
    "                              criterion, optimizer_rotation, NUM_EPOCHS, \n",
    "                              device, \"Rotation Augmentation\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_rotation, \"Rotation Augmentation\")\n",
    "\n",
    "# Evaluate on test set\n",
    "results_rotation = evaluate_model(model_rotation, test_loader, device, CLASS_NAMES)\n",
    "\n",
    "print(f\"\\nRotation Test Accuracy: {results_rotation['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_rotation['confusion_matrix'], CLASS_NAMES, \"Rotation Augmentation\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_rotation.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_rotation.state_dict(),\n",
    "    'history': history_rotation,\n",
    "    'test_results': results_rotation\n",
    "}, os.path.join(MODELS_DIR, 'rotation_model.pth'))\n",
    "\n",
    "print(\"Rotation model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment 3: Horizontal Flip Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3: HORIZONTAL FLIP AUGMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets with flip transform\n",
    "train_dataset_flip = dsets.FashionMNIST(root=DATA_DIR, train=True, \n",
    "                                       download=True, transform=transform_flip)\n",
    "\n",
    "# Split into train/val\n",
    "train_flip, _ = torch.utils.data.random_split(\n",
    "    train_dataset_flip, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_flip = torch.utils.data.DataLoader(train_flip, \n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model_flip = CNN(OUT_CHANNELS_1, OUT_CHANNELS_2)\n",
    "optimizer_flip = optim.SGD(model_flip.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train model\n",
    "history_flip = train_model(model_flip, train_loader_flip, val_loader_baseline,\n",
    "                          criterion, optimizer_flip, NUM_EPOCHS, \n",
    "                          device, \"Horizontal Flip\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_flip, \"Horizontal Flip\")\n",
    "\n",
    "# Evaluate on test set\n",
    "results_flip = evaluate_model(model_flip, test_loader, device, CLASS_NAMES)\n",
    "\n",
    "print(f\"\\nFlip Test Accuracy: {results_flip['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_flip['confusion_matrix'], CLASS_NAMES, \"Horizontal Flip\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_flip.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_flip.state_dict(),\n",
    "    'history': history_flip,\n",
    "    'test_results': results_flip\n",
    "}, os.path.join(MODELS_DIR, 'flip_model.pth'))\n",
    "\n",
    "print(\"Flip model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment 4: Combined Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 4: COMBINED AUGMENTATION (Rotation + Flip + Affine)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create datasets with combined transform\n",
    "train_dataset_combined = dsets.FashionMNIST(root=DATA_DIR, train=True, \n",
    "                                           download=True, transform=transform_combined)\n",
    "\n",
    "# Split into train/val\n",
    "train_combined, _ = torch.utils.data.random_split(\n",
    "    train_dataset_combined, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_combined = torch.utils.data.DataLoader(train_combined, \n",
    "                                                    batch_size=BATCH_SIZE, \n",
    "                                                    shuffle=True)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model_combined = CNN(OUT_CHANNELS_1, OUT_CHANNELS_2)\n",
    "optimizer_combined = optim.SGD(model_combined.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train model\n",
    "history_combined = train_model(model_combined, train_loader_combined, val_loader_baseline,\n",
    "                              criterion, optimizer_combined, NUM_EPOCHS, \n",
    "                              device, \"Combined Augmentation\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_combined, \"Combined Augmentation\")\n",
    "\n",
    "# Evaluate on test set\n",
    "results_combined = evaluate_model(model_combined, test_loader, device, CLASS_NAMES)\n",
    "\n",
    "print(f\"\\nCombined Test Accuracy: {results_combined['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(results_combined['confusion_matrix'], CLASS_NAMES, \"Combined Augmentation\")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_combined.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_combined.state_dict(),\n",
    "    'history': history_combined,\n",
    "    'test_results': results_combined\n",
    "}, os.path.join(MODELS_DIR, 'combined_model.pth'))\n",
    "\n",
    "print(\"Combined model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "\n",
    "experiments = ['Baseline', 'Rotation', 'Horizontal Flip', 'Combined']\n",
    "results_list = [results_baseline, results_rotation, results_flip, results_combined]\n",
    "histories_list = [history_baseline, history_rotation, history_flip, history_combined]\n",
    "\n",
    "# Compile results\n",
    "comparison_data = []\n",
    "for exp_name, results, history in zip(experiments, results_list, histories_list):\n",
    "    comparison_data.append({\n",
    "        'Experiment': exp_name,\n",
    "        'Train Accuracy (%)': f\"{history['train_acc'][-1]:.2f}\",\n",
    "        'Val Accuracy (%)': f\"{history['val_acc'][-1]:.2f}\",\n",
    "        'Test Accuracy (%)': f\"{results['accuracy']*100:.2f}\",\n",
    "        'Final Train Loss': f\"{history['train_loss'][-1]:.4f}\",\n",
    "        'Final Val Loss': f\"{history['val_loss'][-1]:.4f}\",\n",
    "        'Avg Epoch Time (s)': f\"{np.mean(history['epoch_times']):.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv(os.path.join(METRICS_DIR, 'comparison_results.csv'), index=False)\n",
    "print(f\"\\nComparison table saved to {METRICS_DIR}/comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparative accuracy bar chart\n",
    "\n",
    "test_accuracies = [r['accuracy']*100 for r in results_list]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(experiments, test_accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "plt.xlabel('Augmentation Strategy', fontsize=12)\n",
    "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
    "plt.title('Test Accuracy Comparison Across Augmentation Strategies', fontsize=14, fontweight='bold')\n",
    "plt.ylim([min(test_accuracies)-5, max(test_accuracies)+5])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, test_accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.2f}%', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'accuracy_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all training curves together\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Plot validation accuracy for all experiments\n",
    "for exp_name, history, color in zip(experiments, histories_list, colors):\n",
    "    ax1.plot(range(1, NUM_EPOCHS+1), history['val_acc'], \n",
    "            label=exp_name, marker='o', color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "ax1.set_title('Validation Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation loss for all experiments\n",
    "for exp_name, history, color in zip(experiments, histories_list, colors):\n",
    "    ax2.plot(range(1, NUM_EPOCHS+1), history['val_loss'], \n",
    "            label=exp_name, marker='o', color=color, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'all_experiments_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy comparison\n",
    "\n",
    "def get_per_class_accuracy(results):\n",
    "    \"\"\"Extract per-class accuracy from classification report\"\"\"\n",
    "    class_acc = []\n",
    "    for class_name in CLASS_NAMES:\n",
    "        class_acc.append(results['classification_report'][class_name]['recall'] * 100)\n",
    "    return class_acc\n",
    "\n",
    "# Get per-class accuracies for all experiments\n",
    "per_class_data = []\n",
    "for exp_name, results in zip(experiments, results_list):\n",
    "    class_accs = get_per_class_accuracy(results)\n",
    "    per_class_data.append(class_accs)\n",
    "\n",
    "# Plot\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "for i, (exp_name, class_accs, color) in enumerate(zip(experiments, per_class_data, colors)):\n",
    "    ax.bar(x + i*width, class_accs, width, label=exp_name, color=color)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=45, ha='right')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PLOTS_DIR, 'per_class_accuracy_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-class accuracy saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary report\n",
    "\n",
    "summary_report = {\n",
    "    'experiment_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'dataset': 'Fashion-MNIST',\n",
    "    'architecture': {\n",
    "        'type': 'CNN',\n",
    "        'conv1_out_channels': OUT_CHANNELS_1,\n",
    "        'conv2_out_channels': OUT_CHANNELS_2,\n",
    "        'total_parameters': model_baseline.count_parameters()\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'optimizer': 'SGD'\n",
    "    },\n",
    "    'data_split': {\n",
    "        'train_size': train_size,\n",
    "        'val_size': val_size,\n",
    "        'test_size': len(test_dataset)\n",
    "    },\n",
    "    'experiments': {}\n",
    "}\n",
    "\n",
    "# Add results for each experiment\n",
    "for exp_name, results, history in zip(experiments, results_list, histories_list):\n",
    "    summary_report['experiments'][exp_name] = {\n",
    "        'final_train_accuracy': history['train_acc'][-1],\n",
    "        'final_val_accuracy': history['val_acc'][-1],\n",
    "        'test_accuracy': results['accuracy'] * 100,\n",
    "        'final_train_loss': history['train_loss'][-1],\n",
    "        'final_val_loss': history['val_loss'][-1],\n",
    "        'avg_epoch_time': np.mean(history['epoch_times']),\n",
    "        'classification_report': results['classification_report']\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "with open(os.path.join(METRICS_DIR, 'summary_report.json'), 'w') as f:\n",
    "    json.dump(summary_report, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Location: {METRICS_DIR}/summary_report.json\")\n",
    "print(\"\\nAll experiments completed successfully!\")\n",
    "print(f\"\\nBest performing model: {experiments[np.argmax(test_accuracies)]}\")\n",
    "print(f\"Best test accuracy: {max(test_accuracies):.2f}%\")\n",
    "print(f\"\\nImprovement over baseline: {max(test_accuracies) - test_accuracies[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have completed all experiments, you should:\n",
    "\n",
    "1. **Review Results**: Examine all plots and metrics\n",
    "2. **Start Writing Report**: Use the results to write your 5000-word report\n",
    "3. **Critical Analysis**: Analyze why certain augmentations worked better\n",
    "4. **Create Presentation**: Prepare slides based on your findings\n",
    "\n",
    "All your results are saved in:\n",
    "- Models: `../models/`\n",
    "- Plots: `../results/plots/`\n",
    "- Metrics: `../results/metrics/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
